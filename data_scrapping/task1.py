# -*- coding: utf-8 -*-
"""task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y4tWWvS0dLDD55G-W7pOuRMe7FGbf09E
"""

!pip install scrapy
!pip install xlsxwriter

import scrapy
import xlsxwriter
import scrapy.crawler as crawler
from multiprocessing import Process, Queue
from twisted.internet import reactor
from google.colab import files
 
al=[]
bl=[]
cl=[]
dl=[]
 
# your spider
class QuotesSpider(scrapy.Spider):
    name = "news"
    start_urls = ['https://www.energy.gov/listings/energy-news/',
                  'https://www.energy.gov/listings/energy-news?page=1',
                  'https://www.energy.gov/listings/energy-news?page=2',
                  'https://www.energy.gov/listings/energy-news?page=3']
    
 
 
    def parse(self, response):
      for article_date in response.css('.date::text').extract():
          a=(article_date)
          al.append(a)
      for article_headline in response.css('.title-link::text').extract():
          b=(article_headline)
          bl.append(b)
      for article_description in response.css('.odd::text').extract():
          c=(article_description)
          cl.append(c)
      for article_url in response.css('.title-link::attr(href)').extract():
          d=('https://www.energy.gov'+article_url)
          dl.append(d)

     
      print("\nScraping Required Data number 1:\n")
      print(bl[-25:])
      print("\nScraping Required Data number 2:\n")
      print(cl[-25:])
      print("\nScraping Required Data number 3:\n")
      print(dl[-25:])
      print("\nScraping Required Data number 4:\n")
      print(al[-25:])


      print("\n")
      
      xl(al,bl,cl,dl)

def xl(al,bl,cl,dl):
  
  print("\nPlease Scroll this to right to view all data here\n")
  import pandas as pd

  new_list = [al,bl,cl,dl] #a list of list
  df = pd.DataFrame(new_list)
  writer = pd.ExcelWriter('result.xlsx', engine='xlsxwriter')

 #df.to_excel('Path where you want to store the exported excel file\result.xlsx', index = False)
  df.to_excel(writer, index=False)
  writer.save()

run_spider(QuotesSpider)